{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to github: https://github.com/albert-moller/CSSAssignment2.git\n",
    "\n",
    "Group members: Albert Frisch Møller (s214610) and Mark Andrawes (s214654)\n",
    "\n",
    "For this assignment each group member contributed equally to every aspect of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Properties of the real-world network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "with open('social_scientists_network.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "G_real = nx.node_link_graph(data)\n",
    "node_count_real = G_real.number_of_nodes()\n",
    "edge_count_real = G_real.number_of_edges()\n",
    "\n",
    "#Calculate p using the formula p = 2*<L>/(N*(N-1)) obtained by isolating for p in equation 3.2 from Chapter 3\n",
    "\n",
    "p = 2 * edge_count_real / (node_count_real*(node_count_real-1))\n",
    "print(f\"Probability p is {p}\")\n",
    "\n",
    "#Calculate the average degree using the formula <k> = p*(N-1) (equation 3.3 from Chapter 3)\n",
    "\n",
    "average_degree = p*(node_count_real-1)\n",
    "print(f\"Average degree <k> is {average_degree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability p was determined to be 0.000539. The average node degree was determined to be 7.65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a Random Network using node count and probability p\n",
    "\n",
    "import numpy as np\n",
    "import netwulf as nw\n",
    "\n",
    "def random_network(N, p):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            if np.random.uniform(0,1) < p:\n",
    "                G.add_edge(i,j)\n",
    "    return G\n",
    "\n",
    "G_random = random_network(node_count_real, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "connected_components = list(nx.connected_components(G_random))\n",
    "largest_connected_component = max(connected_components, key = len)\n",
    "largest_subgraph_random = G_random.subgraph(largest_connected_component)\n",
    "nw.visualize(largest_subgraph_random)\n",
    "\n",
    "connected_components = list(nx.connected_components(G_real))\n",
    "largest_connected_component = max(connected_components, key = len)\n",
    "largest_subgraph_real = G_real.subgraph(largest_connected_component)\n",
    "nw.visualize(largest_subgraph_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"random_network.png\" alt=\"Random Network\" style=\"width: 450px;\"/>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"real_network.png\" alt=\"Real Network\" style=\"width: 450px;\"/>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute size of the largest connected component of random network.\n",
    "print(f\"Size of the largest connected component (random network) is {largest_subgraph_random.number_of_nodes()}\")\n",
    "\n",
    "#Compute size of the largest connected component of real network.\n",
    "print(f\"Size of the largest connected component (real network) is {largest_subgraph_real.number_of_nodes()}\")\n",
    "\n",
    "#Compute the number of isolated components for the random network\n",
    "isolated_n = list(nx.isolates(G_random))\n",
    "number_of_isolated_nodes = len(isolated_n)\n",
    "print(f\"Number of isolated nodes in the random network: {number_of_isolated_nodes}\")\n",
    "\n",
    "#Compute the number of isolated components for the real network\n",
    "isolated_n = list(nx.isolates(G_real))\n",
    "number_of_isolated_nodes = len(isolated_n)\n",
    "print(f\"Number of isolated nodes in the real network: {number_of_isolated_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What regime does your random network fall into? Is it above or below the critical threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the average node degree of the random network is 7.65, which is significantly greater than 1, the random network falls into the supercritical regime. It is therefore above the critical threshold, which indicates the presence of a giant component.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### According to the textbook, what does the network's structure resemble in this regime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the supercritical regime, the random network has a giant connected component. The size of the largest connected component of the random network was determined to be 14188 and the random network has 8 isolated nodes. The random network's structure resembles that of a fully connected network with few isolated nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on your visualizations, identify the key differences between the actual and the random networks. Explain whether these differences are consistent with theoretical expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random network has less clustering and lacks community structure. Whereas the actual network has more distinct clustering and clear community structures. The random network also has a giant component with some isolated nodes, whereas the actual network is fully connected with no isolated nodes. These differences are consistent with theoretical expectations, as real networks are often characterised by having high clustering, community structures and the presence of hubs. Whereas a Erdős-Rényi random network model is unable to capture these characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Compute degree distribution for the random network:\n",
    "node_degrees_random = [degree for node, degree in G_random.degree()]\n",
    "max_node_degree = max(node_degrees_random)\n",
    "min_node_degree = min(node_degrees_random)\n",
    "bins = np.linspace(min_node_degree, max_node_degree, 20)\n",
    "degree_distribution_random, edges_random = np.histogram(node_degrees_random, bins=bins, density=True)\n",
    "\n",
    "#Compute degree distribution for the real Computational Social Scientists network:\n",
    "node_degrees_real = [degree for node, degree in G_real.degree()]\n",
    "max_node_degree = max(node_degrees_real)\n",
    "min_node_degree = min(node_degrees_real)\n",
    "bins = np.linspace(min_node_degree, max_node_degree, 20)\n",
    "degree_distribution_real, edges_real = np.histogram(node_degrees_real, bins=bins, density=True)\n",
    "\n",
    "# Plot both degree distributions on the same figure using line plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(edges_random[:-1], degree_distribution_random, linestyle='-', marker='o', label='Random Network')\n",
    "plt.plot(edges_real[:-1], degree_distribution_real, linestyle='-', marker='x', label='Computational Social Scientists Network')\n",
    "avg_degree_random = np.mean(node_degrees_random)\n",
    "avg_degree_actual = np.mean(node_degrees_real)\n",
    "plt.axvline(avg_degree_random, color='blue', linestyle='dashed', linewidth=1, label='Avg Degree (Random)')\n",
    "plt.axvline(avg_degree_actual, color='orange', linestyle='dashed', linewidth=2, label='Avg Degree (Actual)')\n",
    "plt.xlabel('Node Degree')\n",
    "plt.ylabel('Log Probability Density')\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Degree Distributions')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does the average degree meaningfully represents the network's characteristics, especially in light of the insights gained from exploring heavy-tailed distributions? Discuss its adequacy or limitations in capturing the essence of the network's structural properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average node degree of a network provides a simple connectivity measure. For networks with heavy tailed distributions, such as the CSS network, the average node is an inadequate measure for capturing the network's structural properties. This is because in these networks most nodes have a degree less than the average node degree, whereas a few nodes known as hubs have a node degree much greater. This means that the average degree can be misleading as it does not capture the presence of highly connected nodes nor the networks distribution spread. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What differences can you observe between the real and the random distributions? How does the shape of the degree distribution for each network inform us about the network's properties? (max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random network, the degree distribution appears to fall off quickly, indicating that many nodes have a node degree close to the average node degree. Therefore the fact that the random networks distribution is centralised around the mean, indicates that the network does not contain large hubs, instead it contains many equally connected nodes. For the real network, the degree distribution has a long tail, which is an indication of a heavy-tailed distribution such as the power law. The power law distribution implies that highly connected nodes (hubs) are present within the network, which is the case for real systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 Network Analysis in Computational Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1: Mixing Patterns and Assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 1: Assortativity Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assortativity_coeffecient(G, attribute: str):\n",
    "    attribute_dict = nx.get_node_attributes(G, attribute)\n",
    "    assert len(attribute_dict) == G.number_of_nodes()\n",
    "    attributes = list(set(attribute_dict.values()))\n",
    "    attribute_indices = {attr: i for i, attr in enumerate(attributes)}\n",
    "    e = np.zeros((len(attributes), len(attributes)))\n",
    "\n",
    "    for a, b in G.edges():\n",
    "        index_a = attribute_indices[attribute_dict[a]]\n",
    "        index_b = attribute_indices[attribute_dict[b]]\n",
    "        e[index_a][index_b] += 1\n",
    "        e[index_b][index_a] += 1\n",
    "    \n",
    "    e = e / e.sum()\n",
    "    a, b = e.sum(axis=1), e.sum(axis=0)\n",
    "    r = (np.sum(np.diag(e)) - np.sum(a * b)) / (1 - np.sum(a * b))\n",
    "    return r \n",
    "        \n",
    "with open('social_scientists_network.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "G_real = nx.node_link_graph(data)\n",
    "\n",
    "r = assortativity_coeffecient(G_real, \"country\")\n",
    "print(\"Assortativity coefficient based on country:\", round(r,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2: Configuration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def configuration_model(G, num_swaps):\n",
    "    RG = G.copy()\n",
    "    edges = list(RG.edges()) \n",
    "    edge_count = len(edges)\n",
    "    for _ in range(num_swaps):\n",
    "        e1_idx, e2_idx = random.sample(range(edge_count), 2)\n",
    "        e1, e2 = edges[e1_idx], edges[e2_idx]\n",
    "        u, v = e1\n",
    "        x, y = e2\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            u, v = v, u\n",
    "\n",
    "        if u != y and v != x and not RG.has_edge(u, y) and not RG.has_edge(x, v):\n",
    "            RG.remove_edge(*e1)\n",
    "            RG.remove_edge(*e2)\n",
    "            RG.add_edge(u, y)\n",
    "            RG.add_edge(x, v)\n",
    "            edges[e1_idx] = (u, y)\n",
    "            edges[e2_idx] = (x, v)\n",
    "\n",
    "    return RG\n",
    "\n",
    "num_swaps = G_real.number_of_edges() * 10\n",
    "randomized_graph = configuration_model(G_real, num_swaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assert that the degrees of the nodes in the original\n",
    "#network and the configuration model remain the same\n",
    "original_degrees = dict(G_real.degree())\n",
    "randomized_degrees = dict(randomized_graph.degree())\n",
    "degrees_match = all(original_degrees[node] == randomized_degrees[node] for node in G_real.nodes())\n",
    "print(f\"Do the degrees match? {degrees_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 3: Analyzing Assortativity in Random Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 100 random networks:\n",
    "assortativity_coeffecients = []\n",
    "num_swaps = G_real.number_of_edges() * 10\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    random_network = configuration_model(G_real, num_swaps)\n",
    "    r = assortativity_coeffecient(random_network, \"country\")\n",
    "    assortativity_coeffecients.append(r)\n",
    "\n",
    "original_assortativity = assortativity_coeffecient(G_real, \"country\")\n",
    "print(f\"Assortativity coeffecient of the original network is {original_assortativity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_assortativity = max(assortativity_coeffecients)\n",
    "min_assortativity = min(assortativity_coeffecients)\n",
    "bins = np.linspace(min_assortativity, max_assortativity, 15)\n",
    "plt.hist(assortativity_coeffecients, bins=bins, color='blue', alpha=0.7)\n",
    "plt.axvline(original_assortativity, color='red', linestyle='dashed', linewidth=1, label = \"original network\")\n",
    "plt.title('Distribution of Assortativity Coefficients')\n",
    "plt.xlabel('Assortativity Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare the results with the assortativity of your original network to determine if connections within the same country are significantly higher than chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot showing the distribution of assortativity coeffecients for 100 random networks and the assortativity coeffecient of the original network, it is clear to see that connections within the same country are indeed significantly higher than chance. This is because the assortativity coeffecient of the original network is signifcantly larger than that of random networks. The large value namely 0.416, indicates that there is a high likelihood that any two nodes from the same country being connected.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4: Assortativity by Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_assortativity(G):\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    # Calculate the joint probability distribution of the degrees of the nodes on either side of an edge\n",
    "    M = np.zeros((max(degrees.values())+1, max(degrees.values())+1))\n",
    "    for u, v in G.edges():\n",
    "        M[degrees[u]][degrees[v]] += 1\n",
    "        M[degrees[v]][degrees[u]] += 1 \n",
    "\n",
    "    # Normalize the matrix\n",
    "    M /= M.sum()\n",
    "    \n",
    "    a_i = M.sum(axis=1)\n",
    "    b_i = M.sum(axis=0)\n",
    "    \n",
    "    assortativity = (np.trace(M) - np.dot(a_i, b_i)) / (1 - np.dot(a_i, b_i))\n",
    "    return assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 100 random networks:\n",
    "degree_assortativities = []\n",
    "num_swaps = G_real.number_of_edges() * 10\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    random_network = configuration_model(G_real, num_swaps)\n",
    "    degree_assort = degree_assortativity(random_network)\n",
    "    degree_assortativities.append(degree_assort)\n",
    "\n",
    "original_degree_assortativity= degree_assortativity(G_real)\n",
    "print(f\"Degree assortativity of the original network is {original_degree_assortativity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_assortativity = max(degree_assortativities)\n",
    "min_assortativity = min(degree_assortativities)\n",
    "bins = np.linspace(min_assortativity, max_assortativity, 15)\n",
    "plt.hist(degree_assortativities, bins=bins, color='blue', alpha=0.7)\n",
    "plt.axvline(original_degree_assortativity, color='red', linestyle='dashed', linewidth=1, label = \"original network\")\n",
    "plt.title('Distribution of Degree Assortativity')\n",
    "plt.xlabel('Degree assortativity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze whether your network shows a tendency for high-degree scientists to connect with other high-degree scientists and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the red dashed line, the degree assortativity of the real computational social scientists network is much higher than the degree assortativity that would be expected by chance. This indicates that high-degree scientists are more likely to connect with other high-degree scientists, which is to be expected for the computational social scientists network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 5: Reflection questions (max 250 words for the 3 questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assortativity by degree. Were the results of the degree assortativity in line with your expectations? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the degree assortativity were in line with our expectations. This is because it is common for researchers who actively publish to colloborate with other active researchers from the same country and field. This means that we generally see a social trend that popular likes to connect with popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Edge flipping. In the process of implementing the configuration model, you were instructed to flip the edges (e.g., changing e_1 from (u,v) to (v,u)) 50% of the time. Why do you think this step is included?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step involving edge flipping is essential as it ensures firstly that the network is undirected and secondly that the network does not inherent directionality nor bias from the original network. This is achieved by edge flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution of assortativity in random networks. Describe the distribution of degree assortativity values you observed for the random networks. Was the distribution pattern expected? Discuss how the nature of random network generation (specifically, the configuration model and edge flipping) might influence this distribution and whether it aligns with theoretical expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of degree assortativity observed for the random networks appears to be concentrated around low assortativity values, close to zero. There is a clear peak in the distribution at degree assortativity values to zero. This distribution pattern is generally to be expected for random networks, due to the random connectivity where high-degree nodes are equally likely to connect to low-degree nodes as they are high-degree nodes. Also, it is attributed to the edge flipping, which reduces the assortativity as it eliminates directionality and inherent bias of the original network. Additionally, theoretically random networks do not have strong assortative mixing by node degree. This means that there is an equally probable chance of an edge forming between similiar degrees and dissimiliar degrees. Therefore, random networks are expected to have a degree assortativity close to zero, which has been observed. Lastly, due to the nature of the configuration model, structural properties of the original network are not taken into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2: Central nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G_real)\n",
    "top5_closeness_centralities = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 scientists by closeness centrality:\")\n",
    "for scientist, centrality in top5_closeness_centralities:\n",
    "    print(f\"Scientist {scientist}: {centrality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What role do you imagine scientists with high closeness centrality play?\n",
    "\n",
    "Given that closeness centrality measures how close a node is to all other nodes in the network, the 5 scientists with highest closeness centrality likely play a critical role in the field. These scientists likely collaborate with many others, and work within various subfield, acting as a bridge between them. As these scientists are closely connected to many others, this indicates that they are very influential, widely recognized and cited - we can imagine that they have produced foundational work which other scientists have built upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(G_real)\n",
    "\n",
    "# Sort and get the top 5\n",
    "sorted_eigenvector = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nTop 5 scientists by eigenvector centrality:\")\n",
    "for scientist, centrality in sorted_eigenvector:\n",
    "    print(f\"Scientist {scientist}: {centrality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G_real.degree())\n",
    "closeness_values = list(closeness_centrality.values())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "# Plotting closeness centrality of nodes vs their degree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(degree_values, closeness_values, color='blue')\n",
    "plt.title('Closeness Centrality vs. Degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Closeness Centrality')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the closeness centrality of nodes vs their degree. Is there a correlation between the two? Did you expect that? Why?\n",
    "\n",
    "The plot seems to show a general trend suggesting that nodes with higher degrees tend to have higher closeness centrality. Generally, we expected there to be a correlation between degree and closeness centrality. This is because the more direct connections a node has with other nodes, the more central the node is in the network - leading to a higher closeness centrality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G_real.degree())\n",
    "closeness_values = list(eigenvector_centrality.values())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(degree_values, closeness_values, color='blue')\n",
    "plt.title('Eigenvector Centrality vs. Degree')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Eigenvector Centrality')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repeat the two points above using eigenvector centrality instead. Do you observe any difference? Why?\n",
    "In this plot, we still see that nodes with higher degrees generally tend to have higher eigenvector centrality, but the relationship has changed. For example, a node with a smaller degree can still achieve a high eigenvector score if it is connected to other nodes that are highly connected or have high eigenvector scores themselves. On the other hand, a node with a high degree connected mostly to nodes with low eigenvector scores will not necessarily have a high eigenvector centrality - this is why we see deviations from the overall trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 - Words that characterize Computational Social Science communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 1: TF-IDF and the Computational Social Science communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What does TF stand for?\n",
    "TF stands for Term Frequency. This is a measure of how often a term occurs in a document. \n",
    "\n",
    "##### What does IDF stand for?\n",
    "IDF stands for Inverse Document Frequency. This is a measure of how important a term is within a corpus of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# load assignment of authors to communities\n",
    "with open('author_to_community_assignment.json', 'r') as file:\n",
    "    author_to_com = json.load(file)\n",
    "\n",
    "communities_df = pd.DataFrame(list(author_to_com.items()), columns=['author_id', 'community'])\n",
    "\n",
    "# load tokenized abstracts\n",
    "tokenized_abstracts =pd.read_csv('abstracts_tokenized.csv')\n",
    "\n",
    "# load papers\n",
    "papers = pd.read_csv('final_papers.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['author_ids'] = papers['author_ids'].apply(literal_eval) \n",
    "papers_exploded = papers.explode('author_ids')\n",
    "\n",
    "authors_in_abstracts = set(communities_df['author_id'])\n",
    "papers_in_abstracts = set(tokenized_abstracts['id'])\n",
    "\n",
    "papers_exploded = papers_exploded[papers_exploded['author_ids'].isin(authors_in_abstracts)]\n",
    "papers_exploded = papers_exploded[papers_exploded['id'].isin(papers_in_abstracts)]\n",
    "\n",
    "papers_exploded['community'] = papers_exploded['author_ids'].apply(lambda x: author_to_com.get(x))\n",
    "\n",
    "tokenized_abstracts['id'] = tokenized_abstracts['id'].astype(str)\n",
    "papers_exploded['id'] = papers_exploded['id'].astype(str)\n",
    "\n",
    "papers_with_abstracts = pd.merge(papers_exploded, tokenized_abstracts[['id', 'tokens']], on='id', how='inner')\n",
    "papers_with_abstracts['tokens'] = papers_with_abstracts['tokens'].apply(literal_eval)\n",
    "community_docs = papers_with_abstracts.groupby('community')['tokens'].apply(lambda tokens_lists: sum(tokens_lists, [])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculating TF\n",
    "def calculate_tf(tokens_list):\n",
    "    tf_dict = Counter(tokens_list)\n",
    "    total_count = len(tokens_list)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] /= total_count\n",
    "    return tf_dict\n",
    "\n",
    "community_docs['tf'] = community_docs['tokens'].apply(calculate_tf)\n",
    "\n",
    "# Find top 5 communities\n",
    "author_counts_per_community = communities_df.groupby('community').size()\n",
    "top_5_communities = author_counts_per_community.sort_values(ascending=False).head(5).index.tolist()\n",
    "top_community_docs = community_docs[community_docs['community'].isin(top_5_communities)].copy()\n",
    "\n",
    "# Get top 5 terms for top communities\n",
    "top_community_docs['top_terms'] = top_community_docs['tf'].apply(lambda x: sorted(x.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "\n",
    "for index, row in top_community_docs.iterrows():\n",
    "    community = row['community']\n",
    "    top_terms = row['top_terms']\n",
    "    print(f\"Community {community}:\")\n",
    "    for term, tf in top_terms:\n",
    "        print(f\" - {term}: {tf}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Describe similarities and differences between the communities.\n",
    "\n",
    "By observing the top 5 terms for each community according to their TF values, we see that the top terms are similar across the 5 communities. Terms like \"the\", \"of\", \"and\", \"to\", and \"a\" are obviously used very often in the documents, and therefore have the greatest TF values out of all the unique tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why aren't the TFs not necessarily a good description of the communities?\n",
    "\n",
    "TFs are not always a good description of the communities as the metric only counts how often words appear in documents but does not account for the context or the significance of those words within the broader corpus. Additionally, as demonstrated before, frequently occuring words like \"the\", \"and\", \"of\" and \"a\" are common across all communities and are not very informative about the specific characteristics of individual communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating IDF\n",
    "import numpy as np\n",
    "doc_freq = Counter(word for tokens in top_community_docs['tokens'] for word in set(tokens))\n",
    "N = len(top_community_docs)\n",
    "idf_values = {word: np.log(N / doc_freq[word]) for word in doc_freq}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What base logarithm did you use? Is that important?\n",
    "\n",
    "In this implementation, the natural logarithm with base e was used. The base used to calculate IDF affects the scale of the values, but it does not influence the relative differences between them. Hence, the base used is not important in terms of finding the terms with the highest IDF in a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Find top 9 communities\n",
    "author_counts_per_community = communities_df.groupby('community').size()\n",
    "top_9_communities = author_counts_per_community.sort_values(ascending=False).head(9).index.tolist()\n",
    "top_9_community_docs = community_docs[community_docs['community'].isin(top_9_communities)].copy()\n",
    "\n",
    "# Calculate TF-IDF\n",
    "def calculate_tfidf(tf_dict, idf_values):\n",
    "    return {word: tf * idf_values.get(word, 0) for word, tf in tf_dict.items()}\n",
    "\n",
    "top_9_community_docs['tfidf'] = top_9_community_docs['tf'].apply(lambda tf: calculate_tfidf(tf, idf_values))\n",
    "\n",
    "aggregate_tf = defaultdict(Counter)\n",
    "aggregate_tfidf = defaultdict(Counter)\n",
    "\n",
    "# Aggregate scores\n",
    "for _, row in top_9_community_docs.iterrows():\n",
    "    aggregate_tf[row['community']].update(row['tf'])\n",
    "    aggregate_tfidf[row['community']].update(row['tfidf'])\n",
    "\n",
    "for community in top_9_communities:\n",
    "    # Sort and select top 10 TF words\n",
    "    top_tf_words = sorted(aggregate_tf[community].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    # Sort and select top 10 TF-IDF words\n",
    "    top_tfidf_words = sorted(aggregate_tfidf[community].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"Community {community}:\")\n",
    "    print(\"Top 10 TF Words:\")\n",
    "    for word, score in top_tf_words:\n",
    "        print(f\"  {word}: {score}\")\n",
    "    print(\"\\nTop 10 TF-IDF Words:\")\n",
    "    for word, score in top_tfidf_words:\n",
    "        print(f\"  {word}: {score}\")\n",
    "    print(\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('social_scientists_network.json', 'r') as file:\n",
    "    network_data = json.load(file)\n",
    "G = nx.node_link_graph(network_data)\n",
    "with open('author_to_community_assignment.json', 'r') as file:\n",
    "    author_to_community = json.load(file)\n",
    "\n",
    "#Find degrees and map to communities\n",
    "author_degrees = dict(G.degree())\n",
    "author_community_df = pd.DataFrame(author_to_community.items(), columns=['author_id', 'community'])\n",
    "author_community_df['degree'] = author_community_df['author_id'].apply(lambda x: author_degrees.get(x, 0))\n",
    "top_9_community_df = author_community_df[author_community_df['community'].isin(top_9_communities)]\n",
    "\n",
    "#get author display names\n",
    "author_names = pd.read_csv('final_authors.csv')\n",
    "author_names.rename(columns={'id': 'author_id'}, inplace=True)\n",
    "author_community_df = author_community_df.merge(author_names, on='author_id', how='left')\n",
    "\n",
    "#get top 3\n",
    "for community in top_9_communities:\n",
    "    top_authors = author_community_df[author_community_df['community'] == community].nlargest(3, 'degree')\n",
    "    print(f\"Top 3 authors in Community {community}:\")\n",
    "    for index, row in top_authors.iterrows():\n",
    "        author_name = row['display_name'] \n",
    "        print(f\"  Author ID: {row['author_id']}, Name: {author_name}, Degree: {row['degree']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "Yes, these words are much better at giving insight into the specific characteristics of each community. For example, it is now clear to see that community 12 deals with the topic of smoking alternatives, which was not clear from the TF-based words. The IDF component makes the TF-IDF words more informative by ensuring that the significance of the terms are not solely based on the number of occurences, but also by how rare they are in the broader corpus, which gives much more insight into unique characteristics of a community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 2: The Wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for community in top_9_communities:\n",
    "    # Select the top 10 TF-IDF words for the community\n",
    "    top_tfidf_words = {word: score for word, score in sorted(aggregate_tfidf[community].items(), key=lambda item: item[1], reverse=True)[:]}\n",
    "    \n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_tfidf_words)\n",
    "    \n",
    "    # Fetch the top 3 authors for this community\n",
    "    top_authors = author_community_df[author_community_df['community'] == community].nlargest(3, 'degree')\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Fetch the names of the top authors\n",
    "    top_authors_names = top_authors['display_name'].tolist()\n",
    "    \n",
    "    # Display the community, the word cloud, and the top authors' names\n",
    "    plt.title(f\"Community {community}\\nTop Authors: {', '.join(top_authors_names)}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3: Computational Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Go back to Week 1, Exercise 1. Revise what you wrote on the topics in Computational Social Science. In light of your data-driven analysis, has your understanding of the field changed? How? (max 150 words)\n",
    "\n",
    "Having conducted a thorough data-driven analysis on the field of CSS, it is evident that our understanding has improved significantly. In week 1, we understood that CSS involved analysing social networks and behaviours using computational methods, like using NLP on social media comments to investigate human interactions. However, having conducted this analysis to effectively investigate the field and its topics, it has become clear that CSS delves into the intricacies of social and human behaviour in much more detail that what we had first imagined. Many of the papers which we have dealt with in this analysis create interesting links between many different aspects, like social networks and online behaviour with biometrics and psychology - this is also why we have seen that many authors with different specialisations have collaborated on papers together (as shown in the social scientists network), since many aspects within the field are interlinked. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
